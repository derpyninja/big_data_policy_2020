{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homeworks - Big Data and Public Policy Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the following homework is to **bring to practice the tools studied in class** by conducting step by step a first big data project. We advise that you do it in parallel with the lectures: the relation between the project steps and the lectures is indicated below. \n",
    "\n",
    "You will build several **machine learning models that will predict continuous and categorical variables based on data coming from webscraping and/or an API**. \n",
    "\n",
    "We propose that you predict the **daily stock price data of a large company** (of your choice) **based on the company's tweets**. You can chose a company from one of the [major stock indices](https://www.wikiwand.com/en/List_of_stock_market_indices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Felix' Proposal:**\n",
    "- predict stock prices of a selected company or aggregate stock index based on tweets related to COVID-19.\n",
    "- Companies:\n",
    "    - Constellation brands which includes Corona (as a gag)\n",
    "    - tourism related companies\n",
    "    - major company producing \"atemschutzmasken\" -> !\n",
    "\n",
    "**MMM**\n",
    "\n",
    "\"Best known as the maker of Scotch tape and Post-It notes, 3M (MMM) also happens to be one of the largest producers of N95 respirators, the type of mask that more efficiently protects people against the virus than ordinary medical masks. Coronavirus has caused a worldwide mask shortage. N95 respirators and regular surgical masks have been unavailable on all major e-commerce platforms in the United States and China since early this year.\n",
    "This would not be the first time that 3M benefited from a global health crisis. When the SARS epidemic hit in 2003, 3M's sales growth shot up amid increased demand for its respirators, according to the Melius report.\n",
    "\"The 2002 impact from SARS was highly beneficial to 3M,\" said Scott Davis, a co-author of the report, told CNN Business. While the company didn't disclose any specifics at the time, \"it was meaningful and helped the stock to outperform in that period.\"\n",
    "Basic medical masks provide a barrier from particulate matter, but do not seal tightly enough against the wearer's face to eliminate the risk of contracting the virus. Worn properly, the N95 mask can filter out about 95% of small airborne particles, according to Christiana Coyle, an expert in epidemics at New York University.\"\n",
    "([See article](https://edition.cnn.com/2020/02/27/business/3m-coronavirus-hedge/index.html)).\n",
    "\n",
    "TODO:\n",
    "- fetch stock price data for 3M as y\n",
    "- fetch tweets about coronovirus and respirators as x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "from pprint import pprint\n",
    "\n",
    "# Display options\n",
    "pd.options.display.max_columns = 10\n",
    "pd.options.display.max_rows = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication OK\n"
     ]
    }
   ],
   "source": [
    "# Credentials\n",
    "api_key = \"gM6qF3EooLId1hTO7jciMugXJ\" # API key\n",
    "api_key_secret = \"4WNbvQj9aZ9jvFMXtxtpe0EXkfrMP3W8EYDwN7Y18UqGVQQGwC\" # API secret key\n",
    "access_token = \"1229695971816288256-KjzKOYMJkY5WqbAYFFqg7OgzyhNcEj\"\n",
    "access_token_secret = \"mmNTwozBE1XQuIM4n3Fg3qEtzanebi9ibb8tWIaE6x2mA\"\n",
    "\n",
    "# Authentification\n",
    "auth = OAuthHandler(api_key, api_key_secret) #creating an OAuthHandler instance\n",
    "auth.set_access_token(access_token, access_token_secret) # set tokens\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)# create api connection\n",
    "\n",
    "# test authentication\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "After having decided the setting (your own or the company + corresponding timespan) that you will study, you should follow the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1a: `X` variables** [week 2]\n",
    "- Fetch the data using the **twitter API** or any other API or website that you are interested in.\n",
    "- Beware of the rate limits and organize your program so as to overcome them if needed\n",
    "- The data should include some text, but might also have other interesting variables (retweets, favorites...)\n",
    "- Create some (non-text based) `X_num` variables that you will use for the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contributors_enabled': False,\n",
      " 'created_at': 'Thu Sep 22 20:12:19 +0000 2011',\n",
      " 'default_profile': False,\n",
      " 'default_profile_image': False,\n",
      " 'description': 'Here, we innovate with purpose & use #science every day to '\n",
      "                'create real impact in every life around the world. '\n",
      "                '#LifeWith3M',\n",
      " 'entities': {'description': {'urls': []},\n",
      "              'url': {'urls': [{'display_url': '3M.com',\n",
      "                                'expanded_url': 'http://www.3M.com',\n",
      "                                'indices': [0, 22],\n",
      "                                'url': 'http://t.co/kRd2k9CCzD'}]}},\n",
      " 'favourites_count': 19542,\n",
      " 'follow_request_sent': False,\n",
      " 'followers_count': 1422186,\n",
      " 'following': False,\n",
      " 'friends_count': 5006,\n",
      " 'geo_enabled': True,\n",
      " 'has_extended_profile': False,\n",
      " 'id': 378197959,\n",
      " 'id_str': '378197959',\n",
      " 'is_translation_enabled': False,\n",
      " 'is_translator': False,\n",
      " 'lang': None,\n",
      " 'listed_count': 1153,\n",
      " 'location': 'St Paul, MN',\n",
      " 'name': '3M',\n",
      " 'notifications': False,\n",
      " 'profile_background_color': 'ABB8C2',\n",
      " 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
      " 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
      " 'profile_background_tile': False,\n",
      " 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/378197959/1571777196',\n",
      " 'profile_image_url': 'http://pbs.twimg.com/profile_images/723375867582287873/mfa49ac6_normal.jpg',\n",
      " 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/723375867582287873/mfa49ac6_normal.jpg',\n",
      " 'profile_link_color': '0084B4',\n",
      " 'profile_location': {'attributes': {},\n",
      "                      'bounding_box': None,\n",
      "                      'contained_within': [],\n",
      "                      'country': '',\n",
      "                      'country_code': '',\n",
      "                      'full_name': 'St Paul, MN',\n",
      "                      'id': '60e2c37980197297',\n",
      "                      'name': 'St Paul, MN',\n",
      "                      'place_type': 'unknown',\n",
      "                      'url': 'https://api.twitter.com/1.1/geo/id/60e2c37980197297.json'},\n",
      " 'profile_sidebar_border_color': 'C0DEED',\n",
      " 'profile_sidebar_fill_color': 'DDEEF6',\n",
      " 'profile_text_color': '333333',\n",
      " 'profile_use_background_image': False,\n",
      " 'protected': False,\n",
      " 'screen_name': '3M',\n",
      " 'status': {'contributors': None,\n",
      "            'coordinates': None,\n",
      "            'created_at': 'Tue Mar 03 21:53:00 +0000 2020',\n",
      "            'entities': {'hashtags': [],\n",
      "                         'symbols': [],\n",
      "                         'urls': [{'display_url': 'twitter.com/i/web/status/1…',\n",
      "                                   'expanded_url': 'https://twitter.com/i/web/status/1234959989892173824',\n",
      "                                   'indices': [107, 130],\n",
      "                                   'url': 'https://t.co/LzgqGByBlX'}],\n",
      "                         'user_mentions': [{'id': 43366773,\n",
      "                                            'id_str': '43366773',\n",
      "                                            'indices': [63, 70],\n",
      "                                            'name': 'Post-it',\n",
      "                                            'screen_name': 'Postit'},\n",
      "                                           {'id': 360831528,\n",
      "                                            'id_str': '360831528',\n",
      "                                            'indices': [85, 92],\n",
      "                                            'name': 'Trello',\n",
      "                                            'screen_name': 'trello'}]},\n",
      "            'favorite_count': 5,\n",
      "            'favorited': False,\n",
      "            'geo': None,\n",
      "            'id': 1234959989892173824,\n",
      "            'id_str': '1234959989892173824',\n",
      "            'in_reply_to_screen_name': None,\n",
      "            'in_reply_to_status_id': None,\n",
      "            'in_reply_to_status_id_str': None,\n",
      "            'in_reply_to_user_id': None,\n",
      "            'in_reply_to_user_id_str': None,\n",
      "            'is_quote_status': False,\n",
      "            'lang': 'en',\n",
      "            'place': None,\n",
      "            'possibly_sensitive': False,\n",
      "            'retweet_count': 0,\n",
      "            'retweeted': False,\n",
      "            'source': '<a href=\"https://www.spredfast.com/\" '\n",
      "                      'rel=\"nofollow\">Khoros</a>',\n",
      "            'text': 'Help turn ideas into results by digitizing your ideas '\n",
      "                    'with the @Postit App, now with @trello integration.… '\n",
      "                    'https://t.co/LzgqGByBlX',\n",
      "            'truncated': True},\n",
      " 'statuses_count': 10264,\n",
      " 'time_zone': None,\n",
      " 'translator_type': 'none',\n",
      " 'url': 'http://t.co/kRd2k9CCzD',\n",
      " 'utc_offset': None,\n",
      " 'verified': True}\n"
     ]
    }
   ],
   "source": [
    "MMM_info = api.get_user(screen_name=\"3M\")\n",
    "pprint(MMM_info._json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       created_at                   id               id_str  \\\n",
      "2  Fri Jan 31 20:36:25 +0000 2020  1223344303193427968  1223344303193427968   \n",
      "1  Fri Feb 28 22:52:57 +0000 2020  1233525525446086658  1233525525446086658   \n",
      "0  Mon Mar 02 23:38:05 +0000 2020  1234624044760227840  1234624044760227840   \n",
      "\n",
      "                                           full_text truncated  ...  \\\n",
      "2  Due to #coronavirus, we are receiving an incre...     False  ...   \n",
      "1  .@CNBC reporter @seemacnbc goes behind-the-sce...     False  ...   \n",
      "0  We are grateful for the efforts of 3M employee...     False  ...   \n",
      "\n",
      "  favorite_count favorited retweeted possibly_sensitive lang  \n",
      "2            136     False     False              False   en  \n",
      "1             58     False     False              False   en  \n",
      "0             59     False     False              False   en  \n",
      "\n",
      "[3 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Fetch data from 3M twitter account\n",
    "target_user = \"3M\"\n",
    "search_tags = ['coronavirus', 'Coronavirus', 'Covid_19']\n",
    "n_tweets = 10\n",
    "\n",
    "#timeline = api.user_timeline(screen_name=\"3M\", count=count, include_rts = True)\n",
    "#tweets = {}\n",
    "#for i, tweet in enumerate(timeline):\n",
    "#    tweets[i] = tweet.text\n",
    "#pd.DataFrame(tweets, index=[\"tweet\"]).transpose()\n",
    "\n",
    "# init empty df\n",
    "df_tweets = pd.DataFrame()\n",
    "\n",
    "# try the following\n",
    "try:\n",
    "    # Fetch nb_tweets_by_target for target\n",
    "    #timeline = api.user_timeline(screen_name=target_user, count=n_tweets, include_rts = True)\n",
    "    \n",
    "    # Put the tweets into a dataframe object\n",
    "    tweet_count=0\n",
    "    for tweet in tweepy.Cursor(api.user_timeline, screen_name='@3M', tweet_mode='extended').items():\n",
    "        # check if a tweet has a hashtag\n",
    "        if len(tweet._json['entities']['hashtags']) > 0:\n",
    "            # check if the tweet contains hashtags specified in \"search_tags\"\n",
    "            tweet_hashtags = [hashtag['text'] for hashtag in tweet._json['entities']['hashtags'] if hashtag['text'] in search_tags]\n",
    "            if(len(tweet_hashtags)>0):\n",
    "                \n",
    "                # 1. Transform the json into a dataframe\n",
    "                df_tweet = pd.DataFrame.from_dict(tweet._json, orient='index', columns=[tweet_count]) # , sleep_on_rate_limit=True\n",
    "\n",
    "                # 2. adds screen name as a row\n",
    "                #df_tweet=df_tweet.append(pd.DataFrame({tweet_count:[target]}, index=['twitter_handle']))\n",
    "\n",
    "                # 3. Add the tweet dataframe to the df_tweets dataframe\n",
    "                df_tweets=pd.concat([df_tweet, df_tweets], axis=1)\n",
    "\n",
    "                # counting the number of target fetched\n",
    "                tweet_count += 1 \n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# except if TweepError arises\n",
    "except tweepy.TweepError: #the error arises when the user has protected tweets\n",
    "    print(\"Failed to run the command on user {}, Skipping...\".format(target_user))\n",
    "\n",
    "# except if RateLimitError arises\n",
    "except tweepy.RateLimitError:\n",
    "    print(\"resource usage limit: {} skipped\".format(target_user))\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "df_tweets = df_tweets.transpose()\n",
    "print(df_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the scraped data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'created_at'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/bd4pp/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'created_at'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5aa2228fc535>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# manually deal with the remaining column dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# TODO: deal with remainings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created_at'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created_at'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# set datetime index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bd4pp/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bd4pp/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'created_at'"
     ]
    }
   ],
   "source": [
    "# try auto-inferring dtypes to cast columns from dtype \"object\" to more appropriate and memory-efficient dtypes\n",
    "df_tweets = df_tweets.infer_objects()\n",
    "\n",
    "# manually deal with the remaining column dtypes\n",
    "# TODO: deal with remainings\n",
    "df_tweets['created_at'] = pd.to_datetime(df_tweets['created_at'])\n",
    "\n",
    "# set datetime index\n",
    "df_tweets = df_tweets.set_index('created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.to_csv(\"./3M_covid19_tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1b: continuous `y` variables** [week 2]\n",
    "- Fetching the data: \n",
    "    - if you work on the suggested idea, you can easily access daily stock prices using the [`yfinance` package](https://pypi.org/project/yfinance/)  (see below)\n",
    "    - otherwise, you can find some interesting data listed in the syllabus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1c: merge `X_num` and `y`** [week 2]\n",
    "- Beware of the temporality: in the case of the proposed study on stock market prices, you will have to deal with the fact that the X is at the tweet level while `y` is daily. \n",
    "\n",
    "***Milestone 1 - March 24th*** *You can submit the previous steps as a first notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample split.** Do the standard 80% / 20% training/test split using all days in the data. In addition, do a separate temporal split where the training set is the first 80% of days in the time series.\n",
    "\n",
    "**For all machine learning models**, report performance measure in test and train samples.\n",
    "\n",
    "**Step 2a: estimate different regression models using `X_num` and `y`** [week 3]\n",
    "\n",
    "***Milestone 2 - March 31th***  *You can submit the previous steps as a second notebook.*\n",
    "\n",
    "**Step 3: text analysis** [week 4]\n",
    "- Featurize tweets (or another text dataset related to your subject): transform the text into a standard document-level dataset `X_doc`\n",
    "\n",
    "**Step 2b: estimate different regression models using `X_doc` and `y`** [week 3]\n",
    "\n",
    "**Step 4: estimate a classification models** [week 5]\n",
    "- propose a categorical variable `y_calc` that you can compute from the continuous one (`y`) (e.g. positive or negative growthin stock prices). For the `X` dimension, you can use `X_doc` or `X_num` or both. \n",
    "- you can use any other categorical variable that you find relevant \n",
    "\n",
    "***Milestone 3 - April 21th***  *You can submit the previous steps as a third notebook.*\n",
    "\n",
    "**Step 6: Dimension reduction** [week 6]\n",
    "- Use one of the dimension reductions methods to dimension-reduce the features\n",
    "    - PCA or topic model (LDA or STM) or k-means clustering on the featurized text `X_doc`\n",
    "- Run another classifier\n",
    "\n",
    "**Step bonus: Econometric identification** [week 8]\n",
    "- Find an exogenous shock affecting this firm (but not all the firms) and a control group of firms not affected\n",
    "    - example: a natural disaster/shock to the exchange rate/change in ownership... affects the functionning of this firm but not the other firms of the stock market index.  \n",
    "- scale up the previous data collection to the firms in the control group\n",
    "- use one of the technique studied in class to causaly identify the impact of the exogenous shock on the stock market of the affected firm\n",
    "\n",
    "***Milestone 4 - May 17th*** *You can submit the previous steps as a last notebook.*\n",
    "\n",
    "---------------------------\n",
    "**Requirement for completion grade**: \n",
    "- the homework should be done in **groups of 1 or 2 persons** \n",
    "- the homework will have to be submitted as a (Python or R) **notebooks**\n",
    "- you can give them back at the indicated **milestone** or on May 17th (no homework will be accepted after May 17th)\n",
    "- the notebooks should contain at least **three graphs** (overall)\n",
    "- each notebook should run from the beginning to then end, but:\n",
    "    - beware of paths to folders that are on your computer but not on mine\n",
    "    - if you have such a path, you can: \n",
    "        - have the data downloaded in the notebook directly\n",
    "        - import your data in a github (or any other online storage) folder that the notebook can access  \n",
    "    - indicate if the notebook takes more than 30 minutes to run\n",
    "- do not write us emails regarding the homework, ask your questions on the **forum**, it will benefit everyone for sure! \n",
    "\n",
    "---------------------------\n",
    "\n",
    "**Alternative homework**\n",
    "\n",
    "If you prefer, we propose for two persons at most to translate all python notebooks of the class into R. This work would count as your homework for the class. Contact us in advance if you want to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the yfinance. \n",
    "# If you get module not found error the run !pip install yfinance from your Jupyter notebook\n",
    "import yfinance as yf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
